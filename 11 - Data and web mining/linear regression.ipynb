{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear regression\n",
    "una regressione lineare identifica una tendenza. È una retta. è la retta che meglio fitta i dati. Chiaramente se il fenomeno non è una retta, il modello è abbastanza inutile. è necessario aumentare l'espressività del modello. posso aumentare il grado del polinomio.\n",
    "In sklearn non si può dire \"fittami un polinomio di grado n\". praticamente crea delle features in più.\n",
    "$ y = b_0 + b_1x + b_2x^2 $\n",
    "se z è la feature x^2, ecco che diventa\n",
    "$ y = b_0 + b_1x + b_2z $, che è a tutti gli effetti una regressione lineare.\n",
    "\n",
    "ovvio che più aumento il grado del polinomio, più fitta perfettamente. il train error scende, il test error esplode.\n",
    "questo per overfitting. impara il trend e tutte le fluttuazioni casuali. oltretuttto un modello troppo complesso genera grandi variazioni in output a poco variare dell'input.\n",
    "\n",
    "## errore modello\n",
    "- **bias** quanto il modello è espressivo per modellare bene i dati (alto polinomio -> poco bias, ugualmente albero con tanti nodi -> poco bias)\n",
    "- **varianza**: quanto il training è stabile\n",
    "\n",
    "perché il modello di regressione con polinomi alti non funziona per nulla? \n",
    "la feature \"giorno\" non è informativa. il minimo sindacabile di feature da usare è il trend dei giorni passati. con una finestra di 10 gg vediamo che l'errore non migliora rispetto alla finestra di 5 giorni. perché? cosa sta facendo il modello? una media mobile. il valore di domani è il valore di oggi più un tot del valore di ieri più un altro tot dei giorni prima.\n",
    "\n",
    "per avere buone predizioni devo avere features adatte. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression e support vector machine\n",
    "trasposizione della regressione lineare per i task di classificazione. La regressione dà un output continuo. mi dà un numero tra 0 e 1, dove 0 è classeA e 1 è classeB. con tutto il range in mezzo che mi aiuta a essere più o meno sicuro.\n",
    "È un modello semplice, funziona abbastanza bene per le classificazioni binarie, è interessante che dia la probabilità perché è un'info in più.\n",
    "\n",
    "potrei però farmi una domanda: quante rette ci sono che dividono le due classi? il numero di modi in cui posso serpararli è potenzialmente infinito. mi devo chiedere qual è la scelta migliore. usando l'esempio, la retta individuata non è certo la migliore. devo guardare la retta che mi dà il margine più grande. \n",
    "Il caso migliore è quando le classificazioni sono separate. nella realtà spesso si sovrappongono un po', in tal caso l'algoritmo cerca di specificare bene l'area di incertezza.\n",
    "il margine è la distanza dei punti più vicini tra le classi opposte dalla decision bound.\n",
    "È interessante il fatto che la massimizzazione del margine implica trovare il modello più semplice (perché minimizzo la norma, che è la radice della somma delle features -> più features sono a 0 più si minimizza -> semplice il modello)\n",
    "\n",
    "se le classi non sono linearmente separabili, posso mettere una misura dell'errore. posso anche scegliere un coefficiente per i due error: preferisco fare qualche errore in più ma avere un ampio margine o al contrario un margine più piccolo ma meno errori? dipende da me e da quello che voglio\n",
    "\n",
    "l'algoritmo funziona bene, ma è mooooolto costoso e permette poche interazioni.\n",
    "\n",
    "# non linearly separable problems? \n",
    "non è detto che la retta sia un buon separatore. \n",
    "L'intuizione è trasformo l'input. magari la feature giusta non è x ma x^2. quindi rimappo a più dimensioni. \n",
    "come progetto un buon kernel (che sarebbe la rimappatura) è un ampissimo ambito di ricerca.\n",
    "\n",
    "È un modello incredibilmente buono, ma ha enormi limitazioni riguardo alla scalabilità, tanto da renderlo intrattabile con dataset grandi"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
